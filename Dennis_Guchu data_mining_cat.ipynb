{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c768d370",
   "metadata": {},
   "source": [
    "# Dennis Guchu\n",
    "# SCT213-C002-0006/2022\n",
    "# Data Mining CAT 1\n",
    "\n",
    "All tasks use the tips dataset (total bill, tip, sex, smoker, day, time, size). Each question must use results from the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32568aa8",
   "metadata": {},
   "source": [
    "(a) Standardize numeric columns (total bill, tip, size) and encode categorical (sex, smoker, day, time). Fit regression total bill = β0 + β1 tip + β2 size + β3 day/time. Diagnose multicollinearity and heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00477dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required dependencies\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Ignore all deprecation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tips dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate numerical vs categorical variables\n",
    "tips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize numeric variables\n",
    "scaler = StandardScaler()\n",
    "\n",
    "tips_scaled = tips.copy()\n",
    "tips_scaled[['total_bill', 'tip', 'size']] = scaler.fit_transform(\n",
    "    tips[['total_bill', 'tip', 'size']]\n",
    ")\n",
    "\n",
    "#encode categorical variables\n",
    "tips_encoded = pd.get_dummies(\n",
    "    tips_scaled,\n",
    "    columns=['sex', 'smoker', 'day', 'time'],\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "tips_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression model\n",
    "X = tips_encoded.drop(columns='total_bill')\n",
    "y = tips_encoded['total_bill']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multicollinearity diagnosis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = X.drop(columns='const').corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix of Predictors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063cc2f",
   "metadata": {},
   "source": [
    "The correlation matrix of the predictors shows that no pair of explanatory variables has a very high correlation (|r| ≥ 0.8), which is the commonly used threshold for severe multicollinearity.\n",
    "\n",
    "Overall, the correlation matrix indicates that multicollinearity is not a serious concern in this regression model, and the estimated coefficients can be interpreted reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5722c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heteroscedasticity diagnosis\n",
    "fitted_vals = model.fittedvalues\n",
    "residuals = model.resid\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(fitted_vals, residuals)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41deaa3",
   "metadata": {},
   "source": [
    "The spread of the residuals increases as the fitted values increase. Specifically, residuals are more tightly clustered around zero at lower fitted values, while a wider dispersion is observed at higher fitted values.\n",
    "\n",
    "This fan-shaped pattern indicates that the variance of the error terms is not constant across all levels of the fitted values, suggesting the presence of heteroscedasticity in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a784755",
   "metadata": {},
   "source": [
    "(b) Using residuals from (a), solve LP: Minimize Z = 4x1 +3x2 ; constraints 3x1 +2x2 ≥ 15, x1 + 4x2 ≥ 16, x1 , x2 ≥ 0. Interpret stochastic fractional allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a8a67",
   "metadata": {},
   "source": [
    "From (a), we fitted:\n",
    "\n",
    "total_bill=β^0+β^1tip+β^2size+β^3(day/time)\n",
    "\n",
    "Residuals:\n",
    "\n",
    "ei=yi−y^i  \n",
    "\n",
    "\n",
    "\n",
    "Objective\n",
    "\n",
    "min Z = 4x_1 + 3x_2  \n",
    "\n",
    "Constraints:\n",
    "\n",
    "3x_1+2x_2≥15\n",
    "\n",
    "X_1+4x_2≥16\n",
    "\n",
    "X_1,x_2≥0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Solve the equalities:\n",
    "\n",
    "3x_1+2x_2=15\n",
    "\n",
    "X_1+4x_2=16\n",
    "\n",
    "\n",
    "\n",
    "Multiply second equation by 3:\n",
    "\n",
    "3x_1+12x_2=48\n",
    "\n",
    "Subtract first equation:\n",
    "\n",
    "(3x_1+12x_2)−(3x_1+2x_2)=48−15\n",
    "\n",
    "10x_2 =33⇒x_2 =3.3  \n",
    "\n",
    "Substitute back:\n",
    "\n",
    "X_1+4(3.3)=16⇒x_1=2.8\n",
    "\n",
    "Optimal Solution\n",
    "\n",
    "X_1=2.8,  x_2=3.3\n",
    "\n",
    "Minimum Cost:\n",
    "\n",
    "Z=4(2.8)+3(3.3)=11.2+9.9=21.1\n",
    "\n",
    "\n",
    "\n",
    "Interpretation\n",
    "\n",
    "x_1 and x_2 are not integers\n",
    "\n",
    "This implies divisible resources, not discrete units\n",
    "\n",
    "\n",
    "\n",
    "Residuals from (a) indicate uncertainty in predicting total bill\n",
    "\n",
    "LP variables represent probabilistic or fractional allocation of corrective effort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183c03b",
   "metadata": {},
   "source": [
    "(c) Based on LP-adjusted predictions from (b), predict total bill and compute confidence/lift for sex=Male ⇒ time=Dinner. Assess robustness under small-sample bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcd7fe",
   "metadata": {},
   "source": [
    "From (b):\n",
    "\n",
    "LP solution:\n",
    "\n",
    "x1 =2.8,x2 =3.3  \n",
    "\n",
    "\n",
    "LP-Adjusted Prediction\n",
    "\n",
    "We adjust the regression prediction using a weighted residual correction:\n",
    "\n",
    "y^ i(LP) =y^ i +λ1 x1 ei +λ2 x2 ei   \n",
    "\n",
    "Since residuals represent unexplained variability, a simplified proportional adjustment is acceptable:\n",
    "\n",
    "y^ i(LP) =y^ i +αei   \n",
    "\n",
    "where\n",
    "\n",
    "α=x1 +x2 / x1 +x2 + 1  = 6.1/ 7.1 ≈0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Total Bill (Male, Dinner)\n",
    "tips_lp = tips_encoded.copy()\n",
    "tips_lp['y_hat'] = model.fittedvalues\n",
    "tips_lp['residual'] = model.resid\n",
    "\n",
    "alpha = (2.8 + 3.3) / (2.8 + 3.3 + 1)\n",
    "tips_lp['y_hat_lp'] = tips_lp['y_hat'] + alpha * tips_lp['residual']\n",
    "\n",
    "#filter male => Dinner\n",
    "male_dinner = tips_lp[\n",
    "    (tips['sex'] == 'Male') &\n",
    "    (tips['time'] == 'Dinner')\n",
    "]\n",
    "\n",
    "predicted_total_bill = male_dinner['y_hat_lp'].mean()\n",
    "\n",
    "#unscale total bill\n",
    "tb_mean = scaler.mean_[0]   # mean of total_bill\n",
    "tb_std  = scaler.scale_[0]  # std of total_bill\n",
    "\n",
    "predicted_total_bill_unscaled = (predicted_total_bill * tb_std + tb_mean)\n",
    "\n",
    "print(f\"$ {predicted_total_bill_unscaled:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61932c",
   "metadata": {},
   "source": [
    "Association Rule:\n",
    "\n",
    "sex = Male ⇒ time = Dinner\n",
    "\n",
    "We now compute confidence and lift.\n",
    "\n",
    "\n",
    "\n",
    "Definitions\n",
    "\n",
    "Let:\n",
    "\n",
    "A: sex = Male\n",
    "\n",
    "B: time = Dinner\n",
    "\n",
    "Confidence\n",
    "\n",
    "Conf(A⇒B)=P(B∣A)  \n",
    "\n",
    "Lift\n",
    "\n",
    "Lift(A⇒B)=P(B∣A)  / P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tips)\n",
    "\n",
    "male = tips[tips['sex'] == 'Male']\n",
    "dinner = tips[tips['time'] == 'Dinner']\n",
    "male_dinner = tips[(tips['sex'] == 'Male') & (tips['time'] == 'Dinner')]\n",
    "\n",
    "confidence = len(male_dinner) / len(male)\n",
    "lift = confidence / (len(dinner) / N)\n",
    "\n",
    "confidence, lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de416c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conf_lift(data):\n",
    "  male = data[data['sex'] == 'Male']\n",
    "  dinner = data[data['time'] == 'Dinner']\n",
    "  male_dinner = data[(data['sex'] == 'Male') & (data['time'] == 'Dinner')]\n",
    "\n",
    "  conf = len(male_dinner) / len(male)\n",
    "  lift = conf / (len(dinner) / len(data))\n",
    "  return conf, lift\n",
    "\n",
    "conf_lift_samples = []\n",
    "\n",
    "for _ in range(50):\n",
    "  sample = tips.sample(frac=0.8, replace=False)\n",
    "  conf_lift_samples.append(compute_conf_lift(sample))\n",
    "\n",
    "conf_lift_samples = np.array(conf_lift_samples)\n",
    "\n",
    "conf_std = conf_lift_samples[:,0].std()\n",
    "lift_std = conf_lift_samples[:,1].std()\n",
    "\n",
    "conf_std, lift_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d06b72",
   "metadata": {},
   "source": [
    "Low standard deviations of confidence and lift across subsamples confirm robustness to small-sample bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7a526",
   "metadata": {},
   "source": [
    "(d) Construct OLAP cube using predicted total bill from (c): day=Region, time=Month. Extract Sun-Dinner and discuss aggregation bias, sparsity, and dimensionality effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bded1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from (c)\n",
    "tips_olap = tips_lp.copy()\n",
    "\n",
    "# Attach conceptual OLAP dimensions\n",
    "tips_olap['Region'] = tips['day']\n",
    "tips_olap['Month'] = tips['time']\n",
    "\n",
    "# OLAP cube via pivot table\n",
    "olap_cube = pd.pivot_table(\n",
    "    tips_olap,\n",
    "    values='y_hat_lp',\n",
    "    index='Region',      # day → Region\n",
    "    columns='Month',     # time → Month\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "olap_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract Sun-Dinner\n",
    "sun_dinner = olap_cube.loc['Sun', 'Dinner']\n",
    "\n",
    "# Recover scaler parameters\n",
    "mean_bill = scaler.mean_[0]\n",
    "std_bill = scaler.scale_[0]\n",
    "\n",
    "sun_dinner_unscaled = mean_bill + sun_dinner * std_bill\n",
    "\n",
    "print(f\"$ {sun_dinner_unscaled:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287bdd9",
   "metadata": {},
   "source": [
    "1. Aggregation bias is introduced when individual-level variation is collapsed into group means\n",
    "\n",
    "High-spending Sunday dinner outliers are averaged with moderate spenders\n",
    "\n",
    "LP-adjusted residual corrections are diluted\n",
    "\n",
    "**Consequence**\n",
    "\n",
    "OLAP value understates extremes\n",
    "\n",
    "Risk of ecological fallacy\n",
    "\n",
    "\n",
    "Aggregation masks within-cell heterogeneity, leading to biased group-level inference.\n",
    "\n",
    "2. **Sparsity**\n",
    "\n",
    "Sun–Lunch → NaN\n",
    "\n",
    "Sat–Lunch → NaN\n",
    "\n",
    "Sparse cells reduce statistical reliability and inflate variance in aggregated estimates.\n",
    "\n",
    "3. Dimensionality Effects\n",
    "\n",
    "If additional dimensions were added:\n",
    "\n",
    "Number of cells explodes\n",
    "\n",
    "More NaNs\n",
    "\n",
    "Lower effective sample size per cell\n",
    "\n",
    "\n",
    "\n",
    "**Result**\n",
    "\n",
    "Curse of dimensionality\n",
    "\n",
    "Trade-off between granularity and robustness\n",
    "\n",
    "\n",
    "\n",
    "Increasing dimensionality improves segmentation but worsens sparsity and aggregation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aae26e",
   "metadata": {},
   "source": [
    "(e) Normalize residual-adjusted columns (size, tip, total bill) from (d). Compare Z-score, Min-Max, robust scaling; quantify impact on clustering stability and regression residual variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f708896",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_e = tips_encoded.copy()\n",
    "\n",
    "# regression outputs from (a)\n",
    "tips_e['residual'] = model.resid\n",
    "tips_e['y_hat'] = model.fittedvalues\n",
    "\n",
    "# residual-adjusted columns\n",
    "tips_e['total_bill_adj'] = tips_e['y_hat'] + alpha * tips_e['residual']\n",
    "tips_e['tip_adj'] = tips_e['tip'] + alpha * tips_e['residual']\n",
    "tips_e['size_adj'] = tips_e['size'] + alpha * tips_e['residual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['total_bill_adj', 'tip_adj', 'size_adj']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "scalers = {\n",
    "    'Z-score': StandardScaler(),\n",
    "    'Min-Max': MinMaxScaler(),\n",
    "    'Robust': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_sets = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "  scaled_sets[name] = pd.DataFrame(\n",
    "      scaler.fit_transform(tips_e[num_cols]),\n",
    "      columns=num_cols,\n",
    "      index=tips_e.index\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f138cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantify cluster stability\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_stability = {}\n",
    "\n",
    "for name, X_scaled in scaled_sets.items():\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    cluster_stability[name] = silhouette_score(X_scaled, labels)\n",
    "\n",
    "cluster_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90076d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantify residual variance\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "residual_variance = {}\n",
    "\n",
    "for name, X_scaled in scaled_sets.items():\n",
    "    X = sm.add_constant(X_scaled[['tip_adj', 'size_adj']])\n",
    "    y = X_scaled['total_bill_adj']\n",
    "\n",
    "    model_scaled = sm.OLS(y, X).fit()\n",
    "    residual_variance[name] = np.var(model_scaled.resid)\n",
    "\n",
    "residual_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d0d99",
   "metadata": {},
   "source": [
    "(f) Perform 2-cluster k-means on normalized data from (e). Compute centroids, silhouette, Davies-Bouldin. Identify misassigned points and relate clusters to regression residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare normalized data\n",
    "X = scaled_sets['Robust'][['total_bill_adj', 'tip_adj', 'size_adj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886049f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 2-Cluster K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "X['cluster'] = cluster_labels\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Clustering Metrics\n",
    "#(i) Silhouette Score\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "sil_score = silhouette_score(X[['total_bill_adj', 'tip_adj', 'size_adj']], X['cluster'])\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d147eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ii) Davies-Bouldin Index\n",
    "db_score = davies_bouldin_score(X[['total_bill_adj', 'tip_adj', 'size_adj']], X['cluster'])\n",
    "db_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify Misassigned Points\n",
    "import numpy as np\n",
    "\n",
    "distances = kmeans.transform(X[['total_bill_adj', 'tip_adj', 'size_adj']])\n",
    "# Distance to assigned centroid\n",
    "assigned_distance = np.array([distances[i, label] for i, label in enumerate(X['cluster'])])\n",
    "# Threshold: top 5% farthest points\n",
    "misassigned_idx = X.index[assigned_distance > np.percentile(assigned_distance, 95)]\n",
    "misassigned_points = X.loc[misassigned_idx]\n",
    "misassigned_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# tips_e has residuals from regression\n",
    "X['residual'] = tips_e['residual']\n",
    "\n",
    "sns.boxplot(x='cluster', y='residual', data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b9f91",
   "metadata": {},
   "source": [
    "(g) Using clusters from (f), cross-validate regression. Compare leave-one-out vs k-fold; quantify variance-bias tradeoff. Evaluate ensemble improvement with correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_reg = sm.add_constant(X_scaled[['tip_adj', 'size_adj']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaled_sets['Robust']\n",
    "y = X_scaled['total_bill_adj']\n",
    "\n",
    "X_reg = sm.add_constant(X_scaled[['tip_adj', 'size_adj']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe65623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave-One-Out Cross-Validation\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "mse_loo = []\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "for train_idx, test_idx in loo.split(X_reg):\n",
    "    lr.fit(X_reg.iloc[train_idx], y.iloc[train_idx])\n",
    "    y_pred = lr.predict(X_reg.iloc[test_idx])\n",
    "    mse_loo.append((y_pred - y.iloc[test_idx].values)**2)\n",
    "\n",
    "mse_loo = np.mean(mse_loo)\n",
    "print(\"LOO MSE:\", mse_loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1397d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_kfold = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_reg):\n",
    "    lr.fit(X_reg.iloc[train_idx], y.iloc[train_idx])\n",
    "    y_pred = lr.predict(X_reg.iloc[test_idx])\n",
    "    mse_kfold.append(mean_squared_error(y.iloc[test_idx], y_pred))\n",
    "\n",
    "mse_kfold = np.mean(mse_kfold)\n",
    "print(\"5-Fold CV MSE:\", mse_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aefdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_mse = 0.025859265273434757\n",
    "kf_mse = 0.0263289515950672\n",
    "percent_diff = (kf_mse - loo_mse) / loo_mse * 100\n",
    "percent_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f71940",
   "metadata": {},
   "source": [
    "Using 5-Fold increases bias slightly (~1.8%) but reduces variance hence a more stable estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns\n",
    "\n",
    "ensemble_preds = np.zeros(len(y))\n",
    "\n",
    "for cluster in X['cluster'].unique():\n",
    "  idx = X['cluster'] == cluster\n",
    "  X_cluster = sm.add_constant(X_scaled.loc[idx, ['tip_adj','size_adj']])\n",
    "  y_cluster = y[idx]\n",
    "\n",
    "  lr.fit(X_cluster, y_cluster)\n",
    "  ensemble_preds[idx] = lr.predict(X_cluster)\n",
    "\n",
    "# Ensemble residual variance\n",
    "ensemble_resid_var = np.var(y - ensemble_preds)\n",
    "print(f\"Ensemble residual variance: {ensemble_resid_var}\")\n",
    "\n",
    "# Global regression\n",
    "X_global = sm.add_constant(X_scaled[['tip_adj','size_adj']])\n",
    "model_global = sm.OLS(y, X_global).fit()\n",
    "global_resid_var = np.var(model_global.resid)\n",
    "print(f\"Global variance: {global_resid_var}\")\n",
    "\n",
    "improvement = global_resid_var - ensemble_resid_var\n",
    "print(f\"Improvement: {improvement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a30044",
   "metadata": {},
   "source": [
    "(h) Design a star schema based on outputs from (g): fact table (total bill, tip, size), dimensions (sex, smoker, day, time). Include slowly changing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293f1d0",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Fact Table\n",
    "CREATE TABLE Fact_Tips (\n",
    "    fact_id INT PRIMARY KEY,\n",
    "    total_bill DECIMAL(10,2),\n",
    "    tip DECIMAL(10,2),\n",
    "    size INT,\n",
    "    sex_id INT,\n",
    "    smoker_id INT,\n",
    "    day_id INT,\n",
    "    time_id INT,\n",
    "    cluster_id INT,\n",
    "    FOREIGN KEY (sex_id) REFERENCES Sex_Dim(sex_id),\n",
    "    FOREIGN KEY (smoker_id) REFERENCES Smoker_Dim(smoker_id),\n",
    "    FOREIGN KEY (day_id) REFERENCES Day_Dim(day_id),\n",
    "    FOREIGN KEY (time_id) REFERENCES Time_Dim(time_id),\n",
    "    FOREIGN KEY (cluster_id) REFERENCES Cluster_Dim(cluster_id)\n",
    ");\n",
    "\n",
    "-- Dimension Tables\n",
    "CREATE TABLE Sex_Dim (\n",
    "    sex_id INT PRIMARY KEY,\n",
    "    sex VARCHAR(10),\n",
    "    effective_start_date DATE,\n",
    "    effective_end_date DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE Smoker_Dim (\n",
    "    smoker_id INT PRIMARY KEY,\n",
    "    smoker VARCHAR(5),\n",
    "    effective_start_date DATE,\n",
    "    effective_end_date DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE Day_Dim (\n",
    "    day_id INT PRIMARY KEY,\n",
    "    day VARCHAR(10),\n",
    "    effective_start_date DATE,\n",
    "    effective_end_date DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE Time_Dim (\n",
    "    time_id INT PRIMARY KEY,\n",
    "    time VARCHAR(10),\n",
    "    effective_start_date DATE,\n",
    "    effective_end_date DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE Cluster_Dim (\n",
    "    cluster_id INT PRIMARY KEY,\n",
    "    cluster_name VARCHAR(20)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fbf615",
   "metadata": {},
   "source": [
    "(i) Apply OLAP on the star schema from (h). Analyze revenue trends by day and time. Detect interactions and heterogeneous variance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create OLAP cube\n",
    "\n",
    "fact_table = X.copy()  # X contains residual-adjusted measures + cluster + residual\n",
    "fact_table = fact_table.merge(tips[['sex', 'smoker', 'day', 'time']], left_index=True, right_index=True)\n",
    "\n",
    "# Create OLAP-style pivot table: total_bill by day and time\n",
    "olap_cube = pd.pivot_table(\n",
    "    fact_table,\n",
    "    values='total_bill_adj',\n",
    "    index='day',   # Day as row\n",
    "    columns='time', # Time as column\n",
    "    aggfunc='mean' # Can also use sum for revenue\n",
    ")\n",
    "\n",
    "olap_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze revenue trends by Day and Time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(olap_cube, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Revenue Trends by Day and Time (OLAP Cube)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aa643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect Interactions (Day × Time)\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Two-way ANOVA\n",
    "formula = 'total_bill_adj ~ C(day) * C(time)'\n",
    "model = ols(formula, data=fact_table).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572625c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect Heterogeneous Variance\n",
    "# Group variance by day and time\n",
    "variance_cube = fact_table.groupby(['day','time'])['total_bill_adj'].var().unstack()\n",
    "variance_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(variance_cube, annot=True, fmt=\".4f\", cmap=\"Reds\")\n",
    "plt.title(\"Variance of Total Bill by Day and Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7ca95",
   "metadata": {},
   "source": [
    "(j) Generate association rules from clustered and OLAP-adjusted data in (i) (e.g., size>3 ⇒ total bill>20). Compute lift, leverage, conviction; discuss overfitting risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset\n",
    "# fact_table from (i)\n",
    "data = fact_table.copy()\n",
    "\n",
    "# Convert numeric columns to categorical for association rules\n",
    "data['size_large'] = data['size_adj'] > 3\n",
    "data['high_total_bill'] = data['total_bill_adj'] > 20\n",
    "data['tip_high'] = data['tip_adj'] > 5  # optional threshold\n",
    "\n",
    "# Include categorical dimensions\n",
    "data['sex'] = data['sex']\n",
    "data['smoker'] = data['smoker']\n",
    "data['day'] = data['day']\n",
    "data['time'] = data['time']\n",
    "data['cluster'] = data['cluster']\n",
    "\n",
    "# Select relevant columns for association rules\n",
    "ar_data = data[['size_large','high_total_bill','tip_high','sex','smoker','day','time','cluster']]\n",
    "\n",
    "# Convert all to strings for one-hot encoding\n",
    "ar_data = ar_data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d57c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot encode\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Convert DataFrame to list of lists for TransactionEncoder\n",
    "transactions = ar_data.apply(lambda x: list(x), axis=1).tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_hot = pd.DataFrame(te_ary, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate frequent itemsets\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df_hot, min_support=0.1, use_colnames=True)\n",
    "frequent_itemsets.sort_values('support', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate association rules\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "rules = rules[['antecedents','consequents','support','confidence','lift','leverage','conviction']]\n",
    "rules.sort_values('lift', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc3a80",
   "metadata": {},
   "source": [
    "**Overfitting risks**\n",
    "\n",
    "1.Small dataset\n",
    "\n",
    "With few transactions, many rules may appear strong purely by chance\n",
    "\n",
    "Confidence/lift can be inflated\n",
    "\n",
    "\n",
    "\n",
    "2.Adding many categorical or derived columns implies combinatorial explosion which in turn implies spurious rules\n",
    "\n",
    "\n",
    "3.Using residual-adjusted measures might exaggerate patterns, especially if residuals were extreme\n",
    "\n",
    "**Mitigation**\n",
    "\n",
    "Increase min_support (e.g., ≥10%)\n",
    "\n",
    "Focus on rules with high lift and meaningful antecedents\n",
    "\n",
    "Cross-validate rules on subsets or holdout data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f33ab8",
   "metadata": {},
   "source": [
    "(k) Apply the Minimum Description Length (MDL) principle using all prior analyses. Select among regression, clustering, and classification models. Quantify model complexity vs data-fit and justify selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed43b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(y)  # number of observations\n",
    "\n",
    "#regression mdl\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_scaled[['tip_adj','size_adj']], y)\n",
    "resid = y - lr.predict(X_scaled[['tip_adj','size_adj']])\n",
    "n_params = X_scaled[['tip_adj','size_adj']].shape[1] + 1  # +1 for intercept\n",
    "\n",
    "# MDL formula (simplified, BIC-style)\n",
    "mdl_regression = (n_params/2) * np.log(n) + (n/2) * np.log(np.var(resid))\n",
    "print(\"Regression MDL:\", mdl_regression)\n",
    "\n",
    "#kmeans mdl\n",
    "k = 2\n",
    "features = X_scaled[['tip_adj','size_adj','total_bill_adj']].shape[1]\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X_scaled[['tip_adj','size_adj','total_bill_adj']])\n",
    "inertia = kmeans.inertia_  # sum of squared distances\n",
    "\n",
    "# MDL = model complexity + data fit\n",
    "mdl_clusters = k * features * np.log(n) + (n/2) * np.log(inertia)\n",
    "print(\"K-Means MDL:\", mdl_clusters)\n",
    "\n",
    "#Association rules mdl\n",
    "# rules DataFrame should have 'antecedents' and 'confidence'\n",
    "n_rules = len(rules)\n",
    "avg_conditions = np.mean([len(a) for a in rules['antecedents']])\n",
    "\n",
    "# Fit: log of confidence (smaller fit → better)\n",
    "fit_ar = -np.sum(np.log(rules['confidence'] + 1e-9))  # add small number to avoid log(0)\n",
    "mdl_rules = n_rules * avg_conditions + fit_ar\n",
    "print(\"Association Rules MDL:\", mdl_rules)\n",
    "\n",
    "#comparison\n",
    "mdl_comparison = pd.DataFrame({\n",
    "    'Model': ['Regression', 'K-Means', 'Association Rules'],\n",
    "    'MDL': [mdl_regression, mdl_clusters, mdl_rules]\n",
    "}).sort_values('MDL')\n",
    "\n",
    "print(\"\\nMDL Comparison:\\n\", mdl_comparison)\n",
    "\n",
    "# Selection: lowest MDL is preferred\n",
    "best_model = mdl_comparison.iloc[0]['Model']\n",
    "print(\"\\nSelected Model by MDL:\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36cf343",
   "metadata": {},
   "source": [
    "| Model             | Complexity | Fit (Residual/Error) | MDL Score |\n",
    "| ----------------- | ---------- | -------------------- | --------- |\n",
    "| Regression        | 3          | 0.025 (resid var)    | -440.99   |\n",
    "| K-Means           | 6          | 690.90 (inertia)     | 690.90    |\n",
    "| Association Rules | 2376       | 0.95 (high conf)     | 2376.64   |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
